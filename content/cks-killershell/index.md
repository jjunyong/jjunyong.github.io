---
emoji: ğŸ§¢
title: '[CKS] Killer shell' 
date: '2023-11-14 21:00:00'
author: jjunyong
tags: k8s
categories: DevOps
---

## Q1. 
You have access to multiple clusters from your main terminal through kubectl contexts. Write all context names into /opt/course/1/contexts, one per line.

From the kubeconfig extract the certificate of user restricted@infra-prod and write it decoded to /opt/course/1/cert.

## Q1 ì •ë‹µ
- File /opt/course/1/contexts ê°€ ëª¨ë“  contextë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ê°€
  ```bash
  k config get-contexts -o name > /opt/course/1/contexts
  ```
- File /opt/course/1/cert ê°€ certë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ê°€
  - kubeconfig íŒŒì¼ì˜ clusterê°€ ì•„ë‹Œ restricted@infra-prodì˜ user ì˜ certë‚´ìš©ì„ base64 -dë¡œ ë””ì½”ë“œí•˜ì—¬ /opt/course/1/certì— ì €ì¥ 

### Q2
Falco is installed with default configuration on node cluster1-node1. Connect using ssh cluster1-node1. Use it to:
- Find a Pod running image nginx which creates unwanted package management processes inside its container.
- Find a Pod running image httpd which modifies /etc/passwd.

Save the Falco logs for case 1 under /opt/course/2/falco.log in format: 
```time-with-nanosconds,container-id,container-name,user-name```
No other information should be in any line. Collect the logs for at least 30 seconds.
Afterwards remove the threads (both 1 and 2) by scaling the replicas of the Deployments that control the offending Pods down to 0.

### Q2 ì •ë‹µ
- /opt/course/2/falco.log ì¡´ì¬í•˜ê³  ì •í™•í•œ ë¡œê·¸ í¬ë§·ì„ ê°€ì§€ê³  ìˆì–´ì•¼ í•¨
  - ë¨¼ì € /etc/falco/falco.yamlì„ ì‚´í´ë³´ë©´ syslogì— falcoì˜ ë¡œê·¸ê°€ ì €ì¥ë˜ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 
  ```
  syslog_output:
    enabled: true
  ```
    - ë”°ë¼ì„œ /var/log/syslog ë˜ëŠ” journalctl -fu falcoë¥¼ í†µí•´ì„œ stdoutìœ¼ë¡œ ë¡œê·¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë¡œê·¸ì—ì„œ í™•ì¸ëœ ë¬¸ì œê°€ ë˜ëŠ” ì»¨í…Œì´ë„ˆ idë¥¼ ì°¾ëŠ”ë‹¤.
    - `crictl pods -id 7a864406b9794` ëª…ë ¹ìœ¼ë¡œ 1,2ë²ˆ ì»¨í…Œì´ë„ˆê°€ ê¸°ë™ ì¤‘ì¸ podì— ëŒ€í•œ ì •ë³´ë¥¼ ì–»ëŠ”ë‹¤. 
  - falco ruleì„ ìˆ˜ì •í•˜ì—¬ ë¬¸ì œì˜ í¬ë§·ìœ¼ë¡œ ë¡œê·¸ê°€ ë‚˜íƒ€ë‚˜ê²Œ ë§Œë“¤ê³ , ë‚˜íƒ€ë‚˜ëŠ” ë¡œê·¸ë¥¼ ë³µì‚¬&ë¶™ì—¬ë„£ê¸°ë¡œ ë¡œì»¬ì˜ /opt/cousre/2/falco.logì— ì €ì¥í•œë‹¤.
- 1ë²ˆ deployment scaled down
  - `k -n team-blue scale deploy webapi --replicas 0`
- 2ë²ˆ deployment scaled down
  - `k -n team-purple scale deploy rating-service --replicas 0`

### Q3
You received a list from the DevSecOps team which performed a security investigation of the k8s cluster1 (workload-prod). The list states the following about the apiserver setup:

Accessible through a NodePort Service
Change the apiserver setup so that:

Only accessible through a ClusterIP Service

### Q3 ì •ë‹µ
- kube-apiserverì˜ yamlíŒŒì¼ì—ì„œ ì•„ë˜ í–‰ì„ ì£¼ì„ ë˜ëŠ” ì‚­ì œ ì²˜ë¦¬ ( ìˆ˜ì • ì „ ë°±ì—… í•„ìˆ˜ )
  - --kubernetes-service-node-port=31000
- ìœ„ ì„¤ì • í›„ì—ë„ `k delete svc kubernetes`ë¥¼ í•´ì£¼ì–´ì•¼ ClusterIP íƒ€ì…ìœ¼ë¡œ ì„œë¹„ìŠ¤ê°€ ì¬ì„±ì„±ë˜ê²Œ ëœë‹¤. 

### Q4
There is Deployment container-host-hacker in Namespace team-red which mounts /run/containerd as a hostPath volume on the Node where it's running. This means that the Pod can access various data about other containers running on the same Node.

To prevent this configure Namespace team-red to enforce the baseline Pod Security Standard. Once completed, delete the Pod of the Deployment mentioned above.

Check the ReplicaSet events and write the event/log lines containing the reason why the Pod isn't recreated into /opt/course/4/logs.

### Q4 ì •ë‹µ
- namespace label ì„¤ì •
  - k edit ns team-red í•´ì„œ `pod-security.kubernetes.io/enforce: baseline # add` ì¶”ê°€
- pod delete 
  - k -n team-red delete pod container-host-hacker-dbf989777-wm8fc 
- /opt/course/4/logì— event log ì €ì¥
  - podê°€ deleteë˜ë©´ rsê°€ ë‹¤ì‹œ podë¥¼ ìƒì„±í•´ì£¼ëŠ”ë°, ì´ê²Œ ì™œ failí•˜ëŠ” ì§€ëŠ” rsë¥¼ ì‚´í´ë´„ìœ¼ë¡œì¨ ì•Œ ìˆ˜ ìˆë‹¤.
    - k -n team-red describe rs container-host-hacker-dbf989777
  - ìœ„ describeì˜ ê²°ê³¼ ì¤‘ `Events:`ì ˆì˜ ë‚´ìš©ì„ /opt/course/4/logsì— ì €ì¥í•œë‹¤.
    ```
    Warning  FailedCreate      2m2s (x9 over 2m40s)  replicaset-controller  (combined from similar events): Error creating: pods "container-host-hacker-dbf989777-kjfpn" is forbidden: violates PodSecurity "baseline:latest": hostPath volumes (volume "containerdata")
    ```

### Q5 : Kube-bench ë¬¸ì œë¡œ í•˜ë¼ëŠ” ëŒ€ë¡œ í•˜ë©´ ë¨
### Q6
- ì•„ë˜ ëª…ë ¹ë“¤ì„ ì´ìš©í•´ì„œ ì–´ë–¤ binaryê°€ ë¬¸ì œê°€ ë˜ëŠ” ì§€ ì°¾ìœ¼ë©´ ëœë‹¤. í•œ letterë§Œ ë‹¤ë¥¸ ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ë°˜ë“œì‹œ uniq ì‚¬ìš©í•˜ê¸° 
  - `sha512sum {binary file}`
  - `cat {file} | uniq`

### Q7 
The Open Policy Agent and Gatekeeper have been installed to, among other things, enforce blacklisting of certain image registries. Alter the existing constraint and/or template to also blacklist images from very-bad-registry.com.

Test it by creating a single Pod using image very-bad-registry.com/image in Namespace default, it shouldn't work.

You can also verify your changes by looking at the existing Deployment untrusted in Namespace default, it uses an image from the new untrusted source. The OPA contraint should throw violation messages for this one.

### Q7 ì •ë‹µ
- gatekeeperì˜ crdë¥¼ ì‚´í´ë³´ê¸° 
  - k get crd
- ì´ ì¤‘ì— constraintë§Œ ë”°ë¡œ ì¡°íšŒí•˜ê¸° 
  - k get constraint
  - ê·¸ë¦¬ê³  ê·¸ì¤‘ì— blacklisting imageì™€ ê´€ë ¨ì´ ìˆì„ ê²ƒ ê°™ì€ constraintì— ëŒ€í•´ ì‚´í´ë³¸ë‹¤. 
    - k edit blacklistimages pod-trusted-images
      ```yaml
        apiVersion: constraints.gatekeeper.sh/v1beta1
        kind: BlacklistImages
        metadata:
        ...
        spec:
          match:
            kinds:
            - apiGroups:
              - ""
              kinds:
              - Pod
      ```
  - ì‚´í´ë³´ë©´ ëª¨ë“  podì— ì ìš©ë˜ê²Œ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ constraintTemplateì„ ì—´ê³  ìˆ˜ì •í•œë‹¤. 
    - `not startswith(image, "very-bad-registry.com/")` ì´ í–‰ì„ regoì— ì¶”ê°€
    - k edit constrainttemplates blacklistimages
      ```yaml
        apiVersion: templates.gatekeeper.sh/v1beta1
        kind: ConstraintTemplate
        metadata:
        ...
        spec:
          crd:
            spec:
              names:
                kind: BlacklistImages
          targets:
          - rego: |
              package k8strustedimages

              images {
                image := input.review.object.spec.containers[_].image
                not startswith(image, "docker-fake.io/")
                not startswith(image, "google-gcr-fake.com/")
                not startswith(image, "very-bad-registry.com/") # ADD THIS LINE
              }

              violation[{"msg": msg}] {
                not images
                msg := "not trusted image!"
              }
            target: admission.k8s.gatekeeper.sh
      ```
- ì´ì œ `k run opa-test --image=very-bad-registry.com/image` ë¡œ í…ŒìŠ¤íŠ¸í•´ë³´ë©´ OPAì—ì„œ ë§‰íˆëŠ” ê±¸ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

### Q8 : Secure K8S dashboard

The Kubernetes Dashboard is installed in Namespace kubernetes-dashboard and is configured to:

Allow users to "skip login"
Allow insecure access (HTTP without authentication)
Allow basic authentication
Allow access from outside the cluster
You are asked to make it more secure by:

Deny users to "skip login"
Deny insecure access, enforce HTTPS (self signed certificates are ok for now)
Add the --auto-generate-certificates argument
Enforce authentication using a token (with possibility to use RBAC)
Allow only cluster internal access

### Q8 ì •ë‹µ
- k -n kubernetes-dashboard get pod,svc ë¡œ ì‚´í´ë³´ê¸° 
- serviceê°€ NodePortë¡œ exposeë˜ì–´ ìˆëŠ” ê±¸ ìœ„ì—ì„œ í™•ì¸í•˜ê³  ë…¸ë“œipí™•ì¸ í›„ 
  - curl http://192.168.100.11:32520 ë¡œ ì ‘ê·¼ë¨ì„ í™•ì¸ ( ì¦‰, unsecure )
- k -n kubernetes-dashboard edit deploy kubernetes-dashboard í•˜ì—¬ secureí•˜ê²Œ ì„¤ì •í•˜ê¸° 
  ```yaml
      template:
        spec:
          containers:
          - args:
            - --namespace=kubernetes-dashboard  
            - --authentication-mode=token        # change or delete, "token" is default
            - --auto-generate-certificates       # add
            #- --enable-skip-login=true          # delete or set to false
            #- --enable-insecure-login           # delete
            image: kubernetesui/dashboard:v2.0.3
            imagePullPolicy: Always
            name: kubernetes-dashboard
  ```
  - k -n kubernetes-dashboard edit svc kubernetes-dashboard ë¥¼ í†µí•´ nodePortë¡œ ì ‘ê·¼ë˜ì§€ ì•Šë„ë¡ ã…‡service ìˆ˜ì •í•˜ê¸°
    ```yaml
     spec:
      clusterIP: 10.107.176.19
      externalTrafficPolicy: Cluster   # delete
      internalTrafficPolicy: Cluster
      ports:
      - name: http
        nodePort: 32513                # delete
        port: 9090
        protocol: TCP
        targetPort: 9090
      - name: https
        nodePort: 32441                # delete
        port: 443
        protocol: TCP
        targetPort: 8443
      selector:
        k8s-app: kubernetes-dashboard
      sessionAffinity: None
      type: ClusterIP                  # change or delete
    status:
      loadBalancer: {}
    ```
### Q9 : AppArmor
Some containers need to run more secure and restricted. There is an existing AppArmor profile located at /opt/course/9/profile for this.

Install the AppArmor profile on Node cluster1-node1. Connect using ssh cluster1-node1.

Add label security=apparmor to the Node

Create a Deployment named apparmor in Namespace default with:
- One replica of image nginx:1.19.2
- NodeSelector for security=apparmor
- Single container named c1 with the AppArmor profile enabled

The Pod might not run properly with the profile enabled. Write the logs of the Pod into /opt/course/9/logs so another team can work on getting the application running.

### Q9 ì •ë‹µ
- ì´ ë¬¸ì œëŠ” mockexamì˜ AppArmorë¬¸ì œì™€ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šì€ë° ë‹¨ì§€ ë¡œì»¬ í™˜ê²½ì—ì„œ node1ë¡œ profileíŒŒì¼ì„ ì˜®ê²¨ì¤˜ì•¼ í•˜ê¸°ì— scp ëª…ë ¹ì„ í™œìš©í•˜ê±°ë‚˜ copy&pasteí•´ì•¼ í•œë‹¤.
  - ```scp /opt/course/9/profile cluster1-node1:~/```
- ëª¨ë“  ì‘ì—…ì´ ëë‚œ í›„ ë¡œê·¸ë¥¼ ë¡œì»¬ íŒŒì¼ì— ì €ì¥ 
  - k logs apparmor-85c65645dc-jbch8 > /opt/course/9/logs

### Q10 : Container Runtime Sandbox gVisor
Team purple wants to run some of their workloads more secure. Worker node cluster1-node2 has container engine containerd already installed and it's configured to support the runsc/gvisor runtime.

Create a RuntimeClass named gvisor with handler runsc.

Create a Pod that uses the RuntimeClass. The Pod should be in Namespace team-purple, named gvisor-test and of image nginx:1.19.2. Make sure the Pod runs on cluster1-node2.

Write the dmesg output of the successfully started Pod into /opt/course/10/gvisor-test-dmesg.

### Q10 Answer
- ì•„ë˜ Runtimeclass ìƒì„± í›„ apply
  ```yaml
  apiVersion: node.k8s.io/v1
    kind: RuntimeClass
    metadata:
      name: gvisor
    handler: runsc
  ```
- k -n team-purple run gvisor-test --image=nginx:1.19.2 --dry-run=client -o yaml > 10_pod.yaml ë¥¼ í•˜ì—¬ ë§Œë“¤ì–´ì§„ íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ 2ê°œ í–‰ì„ ì•„ë˜ì²˜ëŸ¼ ì¶”ê°€í•˜ì—¬ apply
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: null
    labels:
      run: gvisor-test
    name: gvisor-test
    namespace: team-purple
  spec:
    nodeName: cluster1-node2 # add
    runtimeClassName: gvisor   # add
    containers:
    - image: nginx:1.19.2
      name: gvisor-test
      resources: {}
    dnsPolicy: ClusterFirst
    restartPolicy: Always
  status: {}
  ```
- dmesg ë¡œ podê°€ gvisor sandboxê°€ ì ìš©ë˜ì—ˆëŠ” ì§€ í™•ì¸ í›„ outputìœ¼ë¡œ ë¡œê·¸ ì €ì¥í•œë‹¤.
  - k -n team-purple exec gvisor-test -- dmesg > /opt/course/10/gvisor-test-dmesgâ‚©`

### Q11 : Secrets in ETCD

There is an existing Secret called database-access in Namespace team-green.
Read the complete Secret content directly from ETCD (using etcdctl) and store it into /opt/course/11/etcd-secret-content. Write the plain and decoded Secret's value of key "pass" into /opt/course/11/database-password.

### Q11 ì •ë‹µ
- kube-apiserver.yaml íŒŒì¼ì—ì„œ etcd ê´€ë ¨ëœ cert ì„¤ì •ì„ ì°¾ëŠ”ë‹¤.
- ì´ë¥¼ í™œìš©í•˜ì—¬ etcdctl ëª…ë ¹ì–´ë¥¼ ì¨ì„œ ê°’ì„ ì–»ì–´ì™€ì•¼ í•œë‹¤. (kubernetes.io ì°¸ê³ )
  - ETCD ëŠ” `/registry/{type}/{namespace}/{name}` ê²½ë¡œì— ë°ì´í„°ë¥¼ ì €ì¥í•œë‹¤. 
```
- ETCDCTL_API=3 etcdctl \
--cert /etc/kubernetes/pki/apiserver-etcd-client.crt \
--key /etc/kubernetes/pki/apiserver-etcd-client.key \
--cacert /etc/kubernetes/pki/etcd/ca.crt get /registry/secrets/team-green/database-access
```
  - ì´ ê²°ê³¼ë¥¼ /opt/course/11/etcd-secret-contentì— ì €ì¥í•œë‹¤.
- ìœ„ì—ì„œ ì–»ì€ secretì˜ ê²°ê³¼ë¡œë¶€í„° passwordë¥¼ base64 decodeí•˜ì—¬ /opt/course/11/database-passwordì— ì €ì¥í•œë‹¤. 

### Q12

You're asked to investigate a possible permission escape in Namespace restricted. The context authenticates as user restricted which has only limited permissions and shouldn't be able to read Secret values.
<br>
Try to find the password-key values of the Secrets secret1, secret2 and secret3 in Namespace restricted. Write the decoded plaintext values into files /opt/course/12/secret1, /opt/course/12/secret2 and /opt/course/12/secret3.